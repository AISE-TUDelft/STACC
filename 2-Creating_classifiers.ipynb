{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bab868e",
   "metadata": {},
   "source": [
    "# Training Classifier\n",
    "In this Notebook we will:\n",
    "- [Load the data for each of the different classes](#data_collection)\n",
    "- [Load the selected base model and hyperparams](#load_model)\n",
    "- [Train the models](#train_model)\n",
    "- [Evaluate the models](#eval)\n",
    "- [Compare to baselines](#score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d524c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitTrainer\n",
    "from datasets import Dataset\n",
    "from setfit import sample_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "## Workaround for dashes in name\n",
    "from importlib import import_module\n",
    "nlbse_statistics = import_module('code-comment-classification.nlbse_statistics') \n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082794a",
   "metadata": {},
   "source": [
    "<a id='data_collection'></a>\n",
    "## Data collection\n",
    "We first load the data. \n",
    "For each language we create a dataset for each of the seperate category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cca0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['java', 'python', 'pharo']\n",
    "lan_cats = []\n",
    "datasets = {}\n",
    "for lan in langs: # for each language\n",
    "    df = pd.read_csv(f'./code-comment-classification/{lan}/input/{lan}.csv')\n",
    "    df['label'] = df.instance_type\n",
    "    cats = list(map(lambda x: lan + '_' + x, list(set(df.category))))\n",
    "    lan_cats = lan_cats + cats\n",
    "    for cat in list(set(df.category)): # for each category\n",
    "        filtered =  df[df.category == cat]\n",
    "        train_data = Dataset.from_pandas(filtered[filtered.partition == 0])\n",
    "        test_data = Dataset.from_pandas(filtered[filtered.partition == 1])\n",
    "        datasets[f'{lan}_{cat}'] = {'train_data': train_data, 'test_data' : test_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c29938",
   "metadata": {},
   "source": [
    "<a id='load_model'></a>\n",
    "\n",
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1245d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_run.pkl', 'rb') as file:\n",
    "    best_run = pickle.load(file)\n",
    "best_run.hyperparameters['batch_size'] = 8 # Reduce batch size\n",
    "    \n",
    "def model_init(params):\n",
    "    params = params or {}\n",
    "    max_iter = params.get(\"max_iter\", 100)\n",
    "    solver = params.get(\"solver\", \"liblinear\")\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_iter\": max_iter,\n",
    "            \"solver\": solver,\n",
    "        }\n",
    "    }\n",
    "    return SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\", **params)\n",
    "\n",
    "# Create a fresh trainer with hyperparams\n",
    "def load_trainer(train_data, test_data):\n",
    "    trainer = SetFitTrainer(\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        loss_class=CosineSimilarityLoss,\n",
    "        model_init=model_init,\n",
    "        column_mapping={\"comment_sentence\": \"text\", \"label\": \"label\"},\n",
    "    )\n",
    "\n",
    "    trainer.apply_hyperparameters(best_run.hyperparameters, final_model=True)\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b020d",
   "metadata": {},
   "source": [
    "<a id='train_model'></a>\n",
    "\n",
    "\n",
    "## Train Models\n",
    "Train and save a model for each of the categories\n",
    "\n",
    "This will take around 3h per category, so around 2 days in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a865dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training java_deprecation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "***** Running training *****\n",
      "  Num examples = 38620\n",
      "  Num epochs = 6\n",
      "  Total optimization steps = 28968\n",
      "  Total train batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd06ee60873425e80bb39c40351072f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef996bf9eec4111b1c4ec3c1f84b6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/28968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df1f538403847449b8930644fa85599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/28968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model for each cat\n",
    "for lan_cat in lan_cats:\n",
    "    print(f'training {lan_cat}')\n",
    "    train_data = datasets[lan_cat]['train_data']\n",
    "    test_data = datasets[lan_cat]['test_data']\n",
    "    trainer = load_trainer(train_data, test_data)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "#     joblib.dump(trainer, f'./models/{lan_cat}_all-mpnet-base-v2.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ba49a",
   "metadata": {},
   "source": [
    "<a id='eval'></a>\n",
    "\n",
    "## Evaluation\n",
    "Next we evaluate each of our trained models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a3819f2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java_Pointer precision: 0.6, recall: 0.8333333333333334, f1 0.6976744186046512 weighted f1: 0.9147401095108914\n",
      "java_rational precision: 0.543859649122807, recall: 0.8157894736842105, f1 0.6526315789473685 weighted f1: 0.9263439459630449\n",
      "java_Ownership precision: 1.0, recall: 1.0, f1 1.0 weighted f1: 1.0\n",
      "java_Expand precision: 0.6456692913385826, recall: 0.7522935779816514, f1 0.6949152542372881 weighted f1: 0.8483209159519988\n",
      "java_deprecation precision: 0.7037037037037037, recall: 0.8636363636363636, f1 0.7755102040816326 weighted f1: 0.9763213659957574\n",
      "java_summary precision: 0.7471264367816092, recall: 0.7738095238095238, f1 0.760233918128655 weighted f1: 0.9157476952136655\n",
      "java_usage precision: 0.6956521739130435, recall: 0.8421052631578947, f1 0.761904761904762 weighted f1: 0.8320261720389173\n",
      "python_Parameters precision: 0.6956521739130435, recall: 0.7832167832167832, f1 0.7368421052631579 weighted f1: 0.8428864585033696\n",
      "python_Expand precision: 0.4803921568627451, recall: 0.6282051282051282, f1 0.5444444444444444 weighted f1: 0.8327291916875933\n",
      "python_Usage precision: 0.5766871165644172, recall: 0.7768595041322314, f1 0.6619718309859155 weighted f1: 0.8057822213746697\n",
      "python_Summary precision: 0.5698924731182796, recall: 0.6883116883116883, f1 0.623529411764706 weighted f1: 0.8712833387820137\n",
      "python_DevelopmentNotes precision: 0.24615384615384617, recall: 0.48484848484848486, f1 0.326530612244898 weighted f1: 0.8534013774824206\n",
      "pharo_Classreferences precision: 0.29411764705882354, recall: 0.5555555555555556, f1 0.3846153846153846 weighted f1: 0.9485475764545531\n",
      "pharo_Example precision: 0.8947368421052632, recall: 0.8662420382165605, f1 0.8802588996763755 weighted f1: 0.8965573044512065\n",
      "pharo_Keymessages precision: 0.4603174603174603, recall: 0.8529411764705882, f1 0.5979381443298968 weighted f1: 0.877328696557039\n",
      "pharo_Collaborators precision: 0.25, recall: 0.7777777777777778, f1 0.3783783783783784 weighted f1: 0.9203772876971802\n",
      "pharo_Responsibilities precision: 0.6376811594202898, recall: 0.6666666666666666, f1 0.6518518518518518 weighted f1: 0.8679629638477591\n",
      "pharo_Intent precision: 0.8666666666666667, recall: 0.9069767441860465, f1 0.8863636363636364 weighted f1: 0.9716359249364867\n",
      "pharo_Keyimplementationpoints precision: 0.2916666666666667, recall: 0.7368421052631579, f1 0.41791044776119407 weighted f1: 0.870273997456845\n"
     ]
    }
   ],
   "source": [
    "# Score each classifier and write scores to CSV\n",
    "scores = []\n",
    "for lan_cat in lan_cats:\n",
    "    trainer = joblib.load(f'./models/{lan_cat}_all-mpnet-base-v2.joblib')\n",
    "    test_data = datasets[lan_cat]['test_data']\n",
    "    y_hat = trainer.model(test_data['comment_sentence'])\n",
    "    y = test_data['label']\n",
    "    _, fp, fn, tp = confusion_matrix(y_hat, y).ravel()\n",
    "    wf1 = f1_score(y, y_hat, average='weighted')\n",
    "    precision, recall, f1 = nlbse_statistics.get_precision_recall_f1(tp, fp, fn)\n",
    "#     print(f'{lan_cat} precision: {precision}, recall: {recall}, f1 {f1} weighted f1: {wf1}')\n",
    "    scores.append({'lan_cat': lan_cat.lower(),'precision': precision,'recall': recall,'f1': f1,'wf1': wf1})\n",
    "\n",
    "df = pd.DataFrame(scores)\n",
    "# df.sort_values('lan_cat').to_excel('scores.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d45fd3",
   "metadata": {},
   "source": [
    "<a id='score'></a>\n",
    "\n",
    "## Comparision with baseline\n",
    "We compare the weighed f1 scores with the baseline\n",
    "\n",
    "| Language_Category             | Baseline    |             |             |            | Ours      |          |          |            |\n",
    "|-------------------------------|-------------|-------------|-------------|------------|-----------|----------|----------|------------|\n",
    "|                               | Precision   | Recall      | F1          | Weighed F1 | Precision | Recall   | F1       | Weighed F1 |\n",
    "| java_deprecation              | 0           | 0           | 0           | 0,916601   | 0,703704  | 0,863636 | 0,77551  | 0,976321   |\n",
    "| java_expand                   | 0,350515464 | 0,267716535 | 0,303571429 | 0,664627   | 0,645669  | 0,752294 | 0,694915 | 0,848321   |\n",
    "| java_ownership                | 1           | 0,68        | 0,80952381  | 0,982152   | 1         | 1        | 1        | 1          |\n",
    "| java_pointer                  | 0,666666667 | 0,24        | 0,352941176 | 0,836971   | 0,6       | 0,833333 | 0,697674 | 0,91474    |\n",
    "| java_rational                 | 0,62962963  | 0,298245614 | 0,404761905 | 0,880968   | 0,54386   | 0,815789 | 0,652632 | 0,926344   |\n",
    "| java_summary                  | 0,384615385 | 0,287356322 | 0,328947368 | 0,779538   | 0,747126  | 0,77381  | 0,760234 | 0,915748   |\n",
    "| java_usage                    | 0,540983607 | 0,358695652 | 0,431372549 | 0,623095   | 0,695652  | 0,842105 | 0,761905 | 0,832026   |\n",
    "| pharo_classreferences         | 0,333333333 | 0,058823529 | 0,1         | 0,932441   | 0,294118  | 0,555556 | 0,384615 | 0,948548   |\n",
    "| pharo_collaborators           | 0,466666667 | 0,25        | 0,325581395 | 0,907787   | 0,25      | 0,777778 | 0,378378 | 0,920377   |\n",
    "| pharo_example                 | 0,76744186  | 0,434210526 | 0,554621849 | 0,682497   | 0,894737  | 0,866242 | 0,880259 | 0,896557   |\n",
    "| pharo_intent                  | 0,576923077 | 0,333333333 | 0,422535211 | 0,871128   | 0,866667  | 0,906977 | 0,886364 | 0,971636   |\n",
    "| pharo_keyimplementationpoints | 0,178571429 | 0,104166667 | 0,131578947 | 0,79483    | 0,291667  | 0,736842 | 0,41791  | 0,870274   |\n",
    "| pharo_keymessages             | 0,3125      | 0,158730159 | 0,210526316 | 0,761551   | 0,460317  | 0,852941 | 0,597938 | 0,877329   |\n",
    "| pharo_responsibilities        | 0,58974359  | 0,333333333 | 0,425925926 | 0,807558   | 0,637681  | 0,666667 | 0,651852 | 0,867963   |\n",
    "| python_developmentnotes       | 0,171875    | 0,169230769 | 0,170542636 | 0,791947   | 0,246154  | 0,484848 | 0,326531 | 0,853401   |\n",
    "| python_expand                 | 0,263157895 | 0,196078431 | 0,224719101 | 0,717097   | 0,480392  | 0,628205 | 0,544444 | 0,832729   |\n",
    "| python_parameters             | 0,514285714 | 0,223602484 | 0,311688312 | 0,64994    | 0,695652  | 0,783217 | 0,736842 | 0,842886   |\n",
    "| python_summary                | 0,122807018 | 0,075268817 | 0,093333333 | 0,710185   | 0,569892  | 0,688312 | 0,623529 | 0,871283   |\n",
    "| python_usage                  | 0,46875     | 0,18404908  | 0,264317181 | 0,626358   | 0,576687  | 0,77686  | 0,661972 | 0,805782   |\n",
    "| average                       | 0,438866649 | 0,244886382 | 0,30876255  | 0,78617216 | 0,589472  | 0,768706 | 0,654395 | 0,893277   |\n",
    "\n",
    "\n",
    "Finally we calculate our final score: \n",
    "\n",
    "\\begin{align}\n",
    "submission\\_score(model) &= (avg. \\space F_1) \\times 0.75 + (\\% \\space of \\space outperformed \\space categories) \\times 0.25\n",
    "\\end{align}\n",
    "\n",
    "We take the average unweighed f1 (0.654395), and the proportion of outperformed categories (all):\n",
    "\n",
    "\\begin{align}\n",
    "submission\\_score = 0.654395 * 0.75 + 1 * 0.25 = 0.74079625\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c95c3",
   "metadata": {},
   "source": [
    "<a id='hub'></a>\n",
    "\n",
    "## Push to hub\n",
    "\n",
    "Finally we push all of our models to the Hugging Face Hub to make them publically avaliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d2e37f4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java-ownership-classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ccc/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'push_to_hub': pass repo_path_or_name='aalkaswan/java-ownership-classifier', repo_url=None, commit_message='Add SetFit model', organization=None, private=True, api_endpoint=None, token='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', git_user=None, git_email=None, config=None, skip_lfs_files=False as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ccc/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in 'push_to_hub': repo_path_or_name. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Cloning https://huggingface.co/aalkaswan/java-ownership-classifier into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991c5250cbe24bbfabf58225e6d48d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013c3ef850b94e9ead3d9fbc9097fac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model_head.pkl: 100%|##########| 6.83k/6.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/aalkaswan/java-ownership-classifier\n",
      "   52311db..01cf829  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python-usage-classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ccc/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'push_to_hub': pass repo_path_or_name='aalkaswan/python-usage-classifier', repo_url=None, commit_message='Add SetFit model', organization=None, private=True, api_endpoint=None, token='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', git_user=None, git_email=None, config=None, skip_lfs_files=False as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ccc/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in 'push_to_hub': repo_path_or_name. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Cloning https://huggingface.co/aalkaswan/python-usage-classifier into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6beb4cd2e8a543f384ec63b9e73aa95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7218b4cf0103411a8e6329c298137315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model_head.pkl: 100%|##########| 6.83k/6.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/aalkaswan/python-usage-classifier\n",
      "   2f03675..b273747  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pharo-example-classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ccc/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'push_to_hub': pass repo_path_or_name='aalkaswan/pharo-example-classifier', repo_url=None, commit_message='Add SetFit model', organization=None, private=True, api_endpoint=None, token='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', git_user=None, git_email=None, config=None, skip_lfs_files=False as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ccc/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in 'push_to_hub': repo_path_or_name. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Cloning https://huggingface.co/aalkaswan/pharo-example-classifier into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbd1d74c47e4c32a020751817407464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510f9355573a4bfe818649a3e34e868e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model_head.pkl: 100%|##########| 6.83k/6.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/aalkaswan/pharo-example-classifier\n",
      "   56c11b2..aa13b96  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pharo-responsibilities-classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ccc/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'push_to_hub': pass repo_path_or_name='aalkaswan/pharo-responsibilities-classifier', repo_url=None, commit_message='Add SetFit model', organization=None, private=True, api_endpoint=None, token='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', git_user=None, git_email=None, config=None, skip_lfs_files=False as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ccc/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in 'push_to_hub': repo_path_or_name. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Cloning https://huggingface.co/aalkaswan/pharo-responsibilities-classifier into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af23b07d4e242feb370fc52875b251b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b8bdab7a7b491e94b9eaaaaf1fa05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model_head.pkl: 100%|##########| 6.83k/6.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/aalkaswan/pharo-responsibilities-classifier\n",
      "   c8fa046..78696a3  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Push to hub\n",
    "token = 'hf_XXXXXXXXXXX'\n",
    "repo = 'XXXXXXXXXXXXX'\n",
    "for lan_cat in lan_cats:\n",
    "    trainer = joblib.load(f'./models/{lan_cat}_all-mpnet-base-v2.joblib')\n",
    "    name = lan_cat.lower().replace('_','-') + '-classifier'\n",
    "    print(name)\n",
    "    trainer.push_to_hub(f'{repo}/{name}', use_auth_token=token, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c84a00",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this Notebook we created our own classifiers which beat the baseline in every category. We uploaded the models to the Hugging Face Hub as well.\n",
    "\n",
    "![display image](https://github.com/snipe/animated-gifs/blob/master/retro-computers/old-school.gif?raw=true)\n",
    "\n",
    "Please join us in the [next and final Notebook](./3-Inference.ipynb) to see how we can load and use these models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ccc]",
   "language": "python",
   "name": "conda-env-ccc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
